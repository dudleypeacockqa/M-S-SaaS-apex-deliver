# V1.2 Implementation Plan - Execution Guide

**Status**: READY FOR EXECUTION
**Created**: 2025-11-17
**Tech-Spec**: [docs/tech-spec-v1.2.md](../tech-spec-v1.2.md)

## Executive Summary

This document provides the step-by-step execution plan for v1.2 enhancements, designed for autonomous implementation using strict TDD.

**Scope**: 23 TODO resolutions, 90%+ performance, 90%+ coverage, production monitoring

**Estimated Effort**: 46-70 hours over 6-9 weeks

**TDD Mandate**: Every change follows RED â†’ GREEN â†’ REFACTOR cycle

---

## Phase 1: Backend TODO Resolution (8-12 hours)

###  Story 1.1: Dashboard Metrics Real Data

**Files**:
- `backend/app/api/routes/dashboard.py` (MODIFY)
- `backend/tests/api/test_dashboard_metrics.py` (CREATE)

**TDD Cycle**:
1. **RED**: Create test expecting real Deal/Document counts from DB
2. **GREEN**: Replace TODO stubs with SQLAlchemy queries
3. **REFACTOR**: Add Redis caching (5min TTL)

**Implementation**:
```python
# backend/app/api/routes/dashboard.py

from sqlalchemy import func, select
from app.models.deal import Deal
from app.models.document import Document

@router.get("/metrics")
async def get_dashboard_metrics(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    # Replace TODO at line 42
    deals_count = await db.scalar(
        select(func.count(Deal.id))
        .where(Deal.organization_id == current_user.organization_id)
    )

    documents_count = await db.scalar(
        select(func.count(Document.id))
        .where(Document.organization_id == current_user.organization_id)
    )

    return {
        "deals_count": deals_count,
        "documents_count": documents_count,
        "tasks_count": 0,  # Implement when Task model ready
        "recent_activity": []  # Implement in Story 1.2
    }
```

**Caching Layer** (add after queries work):
```python
import json
from app.core.cache import get_redis  # Create if missing

@router.get("/metrics")
async def get_dashboard_metrics(...):
    cache_key = f"metrics:{current_user.organization_id}"
    redis = get_redis()

    # Check cache
    cached = await redis.get(cache_key)
    if cached:
        return json.loads(cached)

    # Query DB (as above)
    metrics = {...}

    # Set cache (5 min TTL)
    await redis.setex(cache_key, 300, json.dumps(metrics))
    return metrics
```

**Tests**:
- `test_get_dashboard_metrics_real_data` - Verify correct counts
- `test_metrics_cache_hit` - Mock Redis, verify cache used
- `test_metrics_cache_miss` - Verify DB queried, cache set

---

### Story 1.2: Community Permission Checks

**Files**:
- `backend/app/api/routes/community.py` (MODIFY lines 437, 452)
- `backend/tests/api/test_community_permissions.py` (CREATE)

**TDD Cycle**:
1. **RED**: Test expecting 403 for non-moderators
2. **GREEN**: Add permission check before moderation actions
3. **REFACTOR**: Extract to reusable decorator

**Implementation**:
```python
# backend/app/api/routes/community.py

async def check_moderator_permission(
    user: User,
    community_id: str,
    db: AsyncSession
) -> None:
    """Verify user has moderator or admin role in community."""
    membership = await db.scalar(
        select(CommunityMember)
        .where(
            CommunityMember.user_id == user.id,
            CommunityMember.community_id == community_id,
            CommunityMember.role.in_(['moderator', 'admin'])
        )
    )
    if not membership:
        raise HTTPException(
            status_code=403,
            detail="Moderator access required"
        )

# Line 437: Pin post (moderator action)
@router.post("/{community_id}/posts/{post_id}/pin")
async def pin_post(
    community_id: str,
    post_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    await check_moderator_permission(current_user, community_id, db)
    # ... rest of implementation

# Line 452: Remove member (moderator action)
@router.delete("/{community_id}/members/{user_id}")
async def remove_member(
    community_id: str,
    user_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    await check_moderator_permission(current_user, community_id, db)
    # ... rest of implementation
```

**Tests**:
- `test_pin_post_as_moderator_succeeds`
- `test_pin_post_as_member_fails_403`
- `test_remove_member_as_admin_succeeds`
- `test_remove_member_as_non_moderator_fails_403`

---

### Story 1.3: Financial Routes Optimization

**Files**:
- `backend/app/api/routes/financial.py` (MODIFY lines 189, 309)
- `backend/tests/api/test_financial_optimization.py` (CREATE)

**Implementation** (line 189):
```python
@router.get("/ratios/latest")
async def get_latest_ratios(
    organization_id: str,
    db: AsyncSession = Depends(get_db)
):
    # Check cache
    cache_key = f"ratios:{organization_id}"
    redis = get_redis()
    cached = await redis.get(cache_key)
    if cached:
        return json.loads(cached)

    # Query FinancialRatio model
    result = await db.scalar(
        select(FinancialRatio)
        .where(FinancialRatio.organization_id == organization_id)
        .order_by(FinancialRatio.created_at.desc())
        .limit(1)
    )

    if not result:
        raise HTTPException(status_code=404, detail="No ratios found")

    ratio_data = result.dict()
    await redis.setex(cache_key, 3600, json.dumps(ratio_data))  # 1 hour TTL
    return ratio_data
```

**Implementation** (line 309):
```python
@router.get("/narratives/latest")
async def get_latest_narrative(
    organization_id: str,
    db: AsyncSession = Depends(get_db)
):
    result = await db.scalar(
        select(FinancialNarrative)
        .where(FinancialNarrative.organization_id == organization_id)
        .order_by(FinancialNarrative.created_at.desc())
        .limit(1)
    )

    if not result:
        raise HTTPException(status_code=404, detail="No narratives found")

    return result.dict()
```

---

### Story 1.4: Document Generation File Creation

**Files**:
- `backend/app/services/document_generation_service.py` (MODIFY line 198)
- `backend/app/api/routes/document_generation.py` (MODIFY lines 686, 802)
- `backend/tests/services/test_document_generation.py` (CREATE)

**Implementation** (service line 198):
```python
# backend/app/services/document_generation_service.py

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from docx import Document as DocxDocument

async def generate_document_file(
    job_id: str,
    content: dict,
    template_id: str,
    file_format: str  # "pdf" or "docx"
) -> str:
    """Generate PDF or DOCX file from content."""
    if file_format == "pdf":
        file_path = await _generate_pdf(content, template_id)
    elif file_format == "docx":
        file_path = await _generate_docx(content, template_id)
    else:
        raise ValueError(f"Unsupported format: {file_format}")

    # Upload to R2/S3
    from app.services.s3_storage_service import upload_file
    s3_path = await upload_file(file_path, f"documents/{job_id}.{file_format}")
    return s3_path

async def _generate_pdf(content: dict, template_id: str) -> str:
    """Generate PDF using ReportLab."""
    file_path = f"/tmp/{uuid4()}.pdf"
    c = canvas.Canvas(file_path, pagesize=letter)

    # Render content (simplified example)
    c.drawString(100, 750, content.get("title", "Document"))
    c.drawString(100, 700, content.get("body", ""))

    c.save()
    return file_path

async def _generate_docx(content: dict, template_id: str) -> str:
    """Generate DOCX using python-docx."""
    file_path = f"/tmp/{uuid4()}.docx"
    doc = DocxDocument()

    doc.add_heading(content.get("title", "Document"), 0)
    doc.add_paragraph(content.get("body", ""))

    doc.save(file_path)
    return file_path
```

**Implementation** (routes line 686):
```python
# Resolve user name
user = await db.get(User, created_by_user_id)
created_by = f"{user.first_name} {user.last_name}" if user else "Unknown"
```

**Implementation** (routes line 802):
```python
# Trigger Celery background task
from app.tasks.document_generation import generate_document_file_task
task = generate_document_file_task.delay(job_id=job.id)
job.celery_task_id = task.id
await db.commit()
```

---

### Story 1.5: Export Queue Service

**Files**:
- `backend/app/services/export_queue_service.py` (CREATE)
- `backend/app/api/routes/export_queue.py` (CREATE)
- `backend/tests/services/test_export_queue_service.py` (CREATE)

**Implementation**:
```python
# backend/app/services/export_queue_service.py

from enum import Enum
import json
from typing import List, Optional
from uuid import uuid4

class JobStatus(str, Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class ExportQueueService:
    def __init__(self, redis_client):
        self.redis = redis_client

    async def create_job(
        self,
        job_type: str,
        organization_id: str,
        user_id: str,
        params: dict
    ) -> dict:
        """Create new export job."""
        job_id = str(uuid4())
        job = {
            "id": job_id,
            "type": job_type,
            "organization_id": organization_id,
            "user_id": user_id,
            "params": params,
            "status": JobStatus.PENDING,
            "created_at": datetime.utcnow().isoformat(),
            "progress": 0
        }

        # Store in Redis (24h TTL)
        await self.redis.setex(
            f"export:job:{job_id}",
            86400,
            json.dumps(job)
        )

        return job

    async def get_job(self, job_id: str) -> Optional[dict]:
        """Retrieve job by ID."""
        data = await self.redis.get(f"export:job:{job_id}")
        return json.loads(data) if data else None

    async def update_job_status(
        self,
        job_id: str,
        status: JobStatus,
        progress: int = None,
        file_url: str = None,
        error: str = None
    ) -> dict:
        """Update job status and progress."""
        job = await self.get_job(job_id)
        if not job:
            raise ValueError(f"Job {job_id} not found")

        job["status"] = status
        if progress is not None:
            job["progress"] = progress
        if file_url:
            job["file_url"] = file_url
        if error:
            job["error"] = error
        job["updated_at"] = datetime.utcnow().isoformat()

        await self.redis.setex(
            f"export:job:{job_id}",
            86400,
            json.dumps(job)
        )
        return job

    async def list_jobs(
        self,
        organization_id: str,
        user_id: Optional[str] = None
    ) -> List[dict]:
        """List all jobs for organization."""
        # Scan Redis for job keys
        pattern = "export:job:*"
        jobs = []

        cursor = 0
        while True:
            cursor, keys = await self.redis.scan(cursor, match=pattern, count=100)
            for key in keys:
                data = await self.redis.get(key)
                if data:
                    job = json.loads(data)
                    if job["organization_id"] == organization_id:
                        if not user_id or job["user_id"] == user_id:
                            jobs.append(job)

            if cursor == 0:
                break

        # Sort by created_at desc
        jobs.sort(key=lambda x: x["created_at"], reverse=True)
        return jobs
```

**API Routes**:
```python
# backend/app/api/routes/export_queue.py

from fastapi import APIRouter, Depends
from app.services.export_queue_service import ExportQueueService, JobStatus

router = APIRouter(prefix="/export-jobs", tags=["export-queue"])

@router.get("")
async def list_export_jobs(
    current_user: User = Depends(get_current_user),
    service: ExportQueueService = Depends(get_export_queue_service)
):
    """List all export jobs for current organization."""
    return await service.list_jobs(current_user.organization_id)

@router.get("/{job_id}")
async def get_export_job(
    job_id: str,
    service: ExportQueueService = Depends(get_export_queue_service)
):
    """Get export job status."""
    job = await service.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    return job

@router.post("/{job_id}/retry")
async def retry_export_job(
    job_id: str,
    service: ExportQueueService = Depends(get_export_queue_service)
):
    """Retry failed export job."""
    await service.update_job_status(job_id, JobStatus.PENDING)
    # Re-trigger Celery task here
    return {"status": "retrying"}

@router.delete("/{job_id}")
async def cancel_export_job(
    job_id: str,
    service: ExportQueueService = Depends(get_export_queue_service)
):
    """Cancel pending export job."""
    await service.update_job_status(job_id, JobStatus.FAILED, error="Canceled by user")
    return {"status": "canceled"}
```

---

## Phase 2: Frontend TODO Resolution (10-14 hours)

### Story 2.1: Bulk Actions API Integration

**Files**:
- `frontend/src/hooks/useBulkActions.ts` (MODIFY lines 84, 180, 220)
- `frontend/src/hooks/useBulkActions.test.ts` (CREATE)

**Implementation**:
```typescript
// frontend/src/hooks/useBulkActions.ts

import { useMutation, useQueryClient } from '@tanstack/react-query';
import axios from 'axios';

export const useBulkActions = () => {
  const queryClient = useQueryClient();

  // Line 84: Archive items
  const archiveItems = useMutation({
    mutationFn: async (ids: string[]) => {
      return axios.post('/api/bulk/archive', { ids });
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['items'] });
    }
  });

  // Line 180: Delete items
  const deleteItems = useMutation({
    mutationFn: async (ids: string[]) => {
      return axios.post('/api/bulk/delete', { ids });
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['items'] });
    }
  });

  // Line 220: Unarchive items
  const unarchiveItems = useMutation({
    mutationFn: async (ids: string[]) => {
      return axios.post('/api/bulk/unarchive', { ids });
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['items'] });
    }
  });

  return {
    archiveItems: archiveItems.mutate,
    deleteItems: deleteItems.mutate,
    unarchiveItems: unarchiveItems.mutate,
    isLoading: archiveItems.isPending || deleteItems.isPending || unarchiveItems.isPending
  };
};
```

---

### Story 2.2: Prospect Kanban Drag & Drop

**Files**:
- `frontend/src/components/master-admin/prospects/ProspectKanban.tsx` (MODIFY line 155)
- `frontend/src/components/master-admin/prospects/ProspectKanban.test.tsx` (CREATE)

**Implementation**:
```tsx
// ProspectKanban.tsx

import { DragDropContext, Droppable, Draggable, DropResult } from '@hello-pangea/dnd';
import { useMutation, useQueryClient } from '@tanstack/react-query';

export const ProspectKanban = () => {
  const queryClient = useQueryClient();

  const updateProspectStage = useMutation({
    mutationFn: async ({ prospectId, stage }: { prospectId: string; stage: string }) => {
      return axios.patch(`/api/prospects/${prospectId}`, { stage });
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['prospects'] });
    }
  });

  const onDragEnd = async (result: DropResult) => {
    if (!result.destination) return;

    const { draggableId, destination } = result;

    // Optimistic update
    await updateProspectStage.mutateAsync({
      prospectId: draggableId,
      stage: destination.droppableId
    });
  };

  return (
    <DragDropContext onDragEnd={onDragEnd}>
      {stages.map(stage => (
        <Droppable key={stage.id} droppableId={stage.id}>
          {(provided, snapshot) => (
            <div
              ref={provided.innerRef}
              {...provided.droppableProps}
              className={snapshot.isDraggingOver ? 'bg-blue-50' : ''}
            >
              {prospects
                .filter(p => p.stage === stage.id)
                .map((prospect, index) => (
                  <Draggable key={prospect.id} draggableId={prospect.id} index={index}>
                    {(provided, snapshot) => (
                      <div
                        ref={provided.innerRef}
                        {...provided.draggableProps}
                        {...provided.dragHandleProps}
                        className={snapshot.isDragging ? 'opacity-50 border-2 border-blue-500' : ''}
                      >
                        <ProspectCard prospect={prospect} />
                      </div>
                    )}
                  </Draggable>
                ))}
              {provided.placeholder}
            </div>
          )}
        </Droppable>
      ))}
    </DragDropContext>
  );
};
```

---

### Story 2.3: Document Workspace Enhancements

**Files**:
- `frontend/src/pages/documents/DocumentWorkspace.tsx` (MODIFY lines 117, 130, 215)
- `frontend/src/components/ui/ConfirmDialog.tsx` (CREATE)

**Implementation** (line 117 - Permissions):
```typescript
const updatePermissions = async (docId: string, permissions: Permission[]) => {
  await axios.patch(`/api/documents/${docId}/permissions`, { permissions });
  queryClient.invalidateQueries({ queryKey: ['document', docId] });
};
```

**Implementation** (line 130 - Audit Log):
```typescript
const logAudit = async (action: string, docId: string) => {
  await axios.post('/api/audit-logs', {
    action,
    resource_type: 'document',
    resource_id: docId,
    timestamp: new Date().toISOString()
  });
};
```

**Implementation** (line 215 - Confirmation):
```typescript
import { useConfirmDialog } from '@/hooks/useConfirmDialog';

const { showConfirmDialog } = useConfirmDialog();

const handleDelete = async (docId: string) => {
  const confirmed = await showConfirmDialog({
    title: 'Delete Document',
    message: 'This action cannot be undone. Are you sure?',
    confirmText: 'Delete',
    variant: 'destructive'
  });

  if (confirmed) {
    await deleteDocument(docId);
  }
};
```

---

### Story 2.4: Export Queue UI

**Files**:
- `frontend/src/components/documents/ExportQueue.tsx` (CREATE)
- `frontend/src/components/documents/ExportJob.tsx` (CREATE)

**Implementation**:
```tsx
// ExportQueue.tsx

import { useQuery } from '@tanstack/react-query';
import { ExportJob } from './ExportJob';

export const ExportQueue = () => {
  const { data: jobs, isLoading } = useQuery({
    queryKey: ['export-jobs'],
    queryFn: async () => {
      const response = await axios.get('/api/export-jobs');
      return response.data;
    },
    refetchInterval: 5000  // Poll every 5 seconds
  });

  if (isLoading) return <div>Loading...</div>;

  return (
    <div className="space-y-4">
      <h2 className="text-2xl font-bold">Export Queue</h2>
      {jobs?.length === 0 ? (
        <p className="text-gray-500">No export jobs</p>
      ) : (
        <div className="space-y-2">
          {jobs?.map(job => (
            <ExportJob key={job.id} job={job} />
          ))}
        </div>
      )}
    </div>
  );
};
```

```tsx
// ExportJob.tsx

import { useMutation, useQueryClient } from '@tanstack/react-query';

const STATUS_COLORS = {
  pending: 'gray',
  processing: 'blue',
  completed: 'green',
  failed: 'red'
};

export const ExportJob = ({ job }) => {
  const queryClient = useQueryClient();

  const retryJob = useMutation({
    mutationFn: async () => {
      return axios.post(`/api/export-jobs/${job.id}/retry`);
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['export-jobs'] });
    }
  });

  return (
    <div className="border rounded-lg p-4">
      <div className="flex justify-between items-center">
        <div>
          <h3 className="font-semibold">{job.type}</h3>
          <p className="text-sm text-gray-500">
            {new Date(job.created_at).toLocaleString()}
          </p>
        </div>
        <div className="flex items-center gap-4">
          <Badge variant={STATUS_COLORS[job.status]}>
            {job.status}
          </Badge>
          {job.status === 'processing' && (
            <ProgressBar value={job.progress} max={100} />
          )}
          {job.status === 'completed' && job.file_url && (
            <Button asChild>
              <a href={job.file_url} download>Download</a>
            </Button>
          )}
          {job.status === 'failed' && (
            <Button onClick={() => retryJob.mutate()}>
              Retry
            </Button>
          )}
        </div>
      </div>
    </div>
  );
};
```

---

## Phase 3: Performance Optimization (10-15 hours)

### Code Splitting

**File**: `frontend/src/App.tsx`

```tsx
// Expand lazy loading to ALL routes
import { lazy, Suspense } from 'react';

const MarketingHome = lazy(() => import('./pages/marketing/MarketingHome'));
const DashboardPage = lazy(() => import('./pages/DashboardPage'));
const DealsPage = lazy(() => import('./pages/deals/DealsPage'));
const DocumentsPage = lazy(() => import('./pages/documents/DocumentsPage'));
// ... all other routes

// Heavy libraries - dynamic import
const Recharts = lazy(() => import('recharts'));
const ReactMarkdown = lazy(() => import('react-markdown'));
```

### Image Optimization

**File**: `vite.config.ts`

```typescript
import { ViteImageOptimizer } from 'vite-plugin-imagemin';

export default defineConfig({
  plugins: [
    react(),
    ViteImageOptimizer({
      // Convert to WebP
      webp: {
        quality: 80
      },
      // Compress PNG
      optipng: {
        optimizationLevel: 7
      }
    })
  ]
});
```

### Bundle Analysis

```bash
npm run build
# Opens bundle visualizer showing chunk sizes
# Identify heavy dependencies and replace/tree-shake
```

---

## Phase 4: Test Coverage Enhancement (20-30 hours)

See tech-spec for detailed testing strategy. Key actions:

1. **Run coverage reports**:
   ```bash
   # Backend
   cd backend && pytest --cov=app --cov-report=html

   # Frontend
   cd frontend && npm run test:coverage
   ```

2. **Identify gaps**: Open `htmlcov/index.html` and `coverage/index.html`

3. **Write tests for uncovered lines** (RED â†’ GREEN cycle)

4. **Target 90%+ both stacks**

---

## Phase 5: Production Polish (5-8 hours)

### Performance Dashboard

**File**: `frontend/src/pages/master-admin/PerformanceDashboard.tsx`

```tsx
import { useQuery } from '@tanstack/react-query';
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';

export const PerformanceDashboard = () => {
  const { data: lighthouseScores } = useQuery({
    queryKey: ['lighthouse-scores'],
    queryFn: async () => {
      const response = await axios.get('/api/performance/lighthouse');
      return response.data;
    }
  });

  return (
    <div className="space-y-8">
      <h1 className="text-3xl font-bold">Performance Dashboard</h1>

      <div className="bg-white p-6 rounded-lg shadow">
        <h2 className="text-xl font-semibold mb-4">Lighthouse Scores (Last 30 Days)</h2>
        <LineChart width={800} height={400} data={lighthouseScores}>
          <CartesianGrid strokeDasharray="3 3" />
          <XAxis dataKey="date" />
          <YAxis domain={[0, 100]} />
          <Tooltip />
          <Legend />
          <Line type="monotone" dataKey="performance" stroke="#8884d8" name="Performance" />
          <Line type="monotone" dataKey="accessibility" stroke="#82ca9d" name="Accessibility" />
          <Line type="monotone" dataKey="best_practices" stroke="#ffc658" name="Best Practices" />
          <Line type="monotone" dataKey="seo" stroke="#ff7300" name="SEO" />
        </LineChart>
      </div>

      {/* API Response Times, Error Rates, etc. */}
    </div>
  );
};
```

---

## Deployment

### Checklist

- [ ] All tests passing
- [ ] Coverage â‰¥90% (backend + frontend)
- [ ] Lighthouse Performance â‰¥90%
- [ ] Build succeeds
- [ ] Smoke tests pass

### Commands

```bash
# Commit
git add .
git commit -m "feat(v1.2): complete enhancement epic

- Resolve 23 TODOs (backend + frontend)
- Achieve 90%+ test coverage (3,141 tests)
- Optimize performance to Lighthouse 90%+
- Add export queue UI and monitoring dashboard

ðŸ¤– Generated with Claude Code
Co-Authored-By: Claude <noreply@anthropic.com>"

# Push (triggers Render auto-deploy)
git push origin main

# Create release
gh release create v1.2.0 \
  --title "v1.2.0 - Polish & Optimization" \
  --notes-file docs/bmad/RELEASE-NOTES-v1.2.md

# Tag
git tag -a v1.2.0 -m "Release v1.2.0"
git push origin v1.2.0
```

---

## Success Criteria

âœ… **Code Quality**:
- 0 TODOs remaining
- All tests passing
- Coverage â‰¥90% both stacks

âœ… **Performance**:
- Lighthouse Performance â‰¥90%
- FCP <1.5s, TTI <3.5s
- Bundle size <500KB gzipped

âœ… **Functionality**:
- Dashboard shows real metrics
- Export queue UI functional
- Drag-and-drop works
- All permissions enforced

âœ… **Production**:
- Services healthy on Render
- No critical Sentry errors
- Monitoring dashboards live

---

**This implementation plan is READY FOR EXECUTION.**

Use strict TDD (RED â†’ GREEN â†’ REFACTOR) for every change.

Estimated completion: 46-70 hours over 6-9 weeks.
